{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchaudio==0.10.1 in /home/ubuntu/.local/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: attention in /home/ubuntu/.local/lib/python3.8/site-packages (4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.3.1)\n",
      "Requirement already satisfied: h5py in /usr/lib/python3/dist-packages (2.10.0)\n",
      "Requirement already satisfied: nvidia_smi in /home/ubuntu/.local/lib/python3.8/site-packages (0.1.3)\n",
      "Requirement already satisfied: keras==2.9.0rc1 in /home/ubuntu/.local/lib/python3.8/site-packages (2.9.0rc1)\n",
      "Requirement already satisfied: tensorflow in /home/ubuntu/.local/lib/python3.8/site-packages (2.9.1)\n",
      "Requirement already satisfied: librosa in /home/ubuntu/.local/lib/python3.8/site-packages (0.9.1)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (3.5.2)\n",
      "Requirement already satisfied: pickle5 in /home/ubuntu/.local/lib/python3.8/site-packages (0.0.11)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (7.0.0)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (0.25.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (1.22.4)\n",
      "Requirement already satisfied: opencv-python in /home/ubuntu/.local/lib/python3.8/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numba in /home/ubuntu/.local/lib/python3.8/site-packages (0.55.2)\n",
      "Requirement already satisfied: torch==1.10.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from torchaudio==0.10.1) (1.10.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from nvidia_smi) (1.14.0)\n",
      "Requirement already satisfied: pytest>=4.3.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia_smi) (7.1.2)\n",
      "Requirement already satisfied: sorcery>=0.1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from nvidia_smi) (0.2.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow) (1.29.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (14.0.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from tensorflow) (20.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.11.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/lib/python3/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.6.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/lib/python3/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: resampy>=0.2.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (0.2.2)\n",
      "Requirement already satisfied: audioread>=2.1.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (2.1.9)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/lib/python3/dist-packages (from librosa) (0.22.2.post1)\n",
      "Requirement already satisfied: pooch>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (1.6.0)\n",
      "Requirement already satisfied: soundfile>=0.10.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (0.10.3.post1)\n",
      "Requirement already satisfied: decorator>=4.0.10 in /usr/lib/python3/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (1.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/ubuntu/.local/lib/python3.8/site-packages (from numba) (0.38.1)\n",
      "Requirement already satisfied: py>=1.8.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytest>=4.3.1->nvidia_smi) (1.11.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from pytest>=4.3.1->nvidia_smi) (19.3.0)\n",
      "Requirement already satisfied: iniconfig in /home/ubuntu/.local/lib/python3.8/site-packages (from pytest>=4.3.1->nvidia_smi) (1.1.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytest>=4.3.1->nvidia_smi) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytest>=4.3.1->nvidia_smi) (2.0.1)\n",
      "Requirement already satisfied: littleutils>=0.2.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from sorcery>=0.1.0->nvidia_smi) (0.2.2)\n",
      "Requirement already satisfied: asttokens in /home/ubuntu/.local/lib/python3.8/site-packages (from sorcery>=0.1.0->nvidia_smi) (2.0.5)\n",
      "Requirement already satisfied: executing in /home/ubuntu/.local/lib/python3.8/site-packages (from sorcery>=0.1.0->nvidia_smi) (0.8.3)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/lib/python3/dist-packages (from pooch>=1.0->librosa) (1.4.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.0)\n",
      "Requirement already satisfied: pycparser in /home/ubuntu/.local/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio==0.10.1 attention pyyaml h5py nvidia_smi keras==2.9.0rc1 tensorflow librosa matplotlib pickle5 Pillow pandas numpy opencv-python numba #gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.docs import doc_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6cTyZVydgca",
    "outputId": "222bd80d-a8df-41e6-cb7b-316a13a7e8d2"
   },
   "outputs": [],
   "source": [
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import pickle5 as pickle\n",
    "from PIL import Image as im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4NblImFddp9T"
   },
   "outputs": [],
   "source": [
    "pathG='../data' \n",
    "DATASET_PATH0 = pathG+\"/w2v2/w2v2L0\"\n",
    "DATASET_PATH4 = pathG+\"/w2v2/w2v2L4\"\n",
    "DATASET_PATH8 = pathG+\"/w2v2/w2v2L8\"\n",
    "DATASET_PATHS = [DATASET_PATH0,DATASET_PATH4,DATASET_PATH8]\n",
    "DATASET_PATH = DATASET_PATHS[0]\n",
    "#https://stackoverflow.com/questions/10443295/combine-3-separate-numpy-arrays-to-an-rgb-image-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "therapist='Yared Alemu'\n",
    "emotion='fear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dyfp0YeCwBMJ"
   },
   "outputs": [],
   "source": [
    "# Get data list\n",
    "\"\"\"\n",
    "Created on Tue Jun  7 20:48:17 2022\n",
    "\n",
    "@author: sanjeev\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization #, regularizers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import models_custom as mcs\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "#os.chdir('/media/sanjeev/Data/Pract/Practicum/codesNdata/mycode')\n",
    "#pathG='../data'\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(pathG+\"/labels/\"+therapist+\"_\"+emotion+\".csv\")\n",
    "df = df.reset_index()\n",
    "df['name2'] =df['name'].apply (lambda x: x.split(\".\")[0]+\".pickle\")\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "import os\n",
    " \n",
    "filenames= glob.glob(DATASET_PATH+\"/*.pickle\" )\n",
    "filenames = [os.path.basename(x) for x in filenames]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TTNcFKd2FNaG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is to delete the rows greater than max size\n",
    "l95=5000\n",
    "df2=df.copy(deep=True)\n",
    "for i,row in df.iterrows():    \n",
    "    fl = DATASET_PATH+\"/\"+row['name2']\n",
    "    try:\n",
    "        with open(fl,\"rb\") as f:\n",
    "            x=pickle.load(f)     \n",
    "        l1 = x[0].shape[0]\n",
    "        w1 = x[0].shape[1] \n",
    "        if l1> l95:\n",
    "            try:\n",
    "            #print(i)\n",
    "                df2=df2.drop(df.iloc[i].name)\n",
    "            except:\n",
    "                print(i,l1)\n",
    "                print(\"encountered and error\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df= df2.reset_index()\n",
    "x=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "hqEZFlE-R3fW",
    "outputId": "69aee205-8c26-4dca-e7c2-91e9b40c9243"
   },
   "outputs": [],
   "source": [
    "fl='test.pickle'\n",
    "with open(fl,\"rb\") as f:\n",
    "    x=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "z_Nby6tPCYiM"
   },
   "outputs": [],
   "source": [
    "# Normalizing data and creating an array\n",
    "def create_arr(fl,l95, height,width,img):\n",
    "    if img==1:\n",
    "        pxl=255\n",
    "    else:\n",
    "        pxl=1  \n",
    "    with open(fl,\"rb\") as f:\n",
    "        x=pickle.load(f) \n",
    "    l1 = x[0].shape[0]\n",
    "    try:\n",
    "        #print(l95-l1+1)\n",
    "        x=np.pad(x.cpu().detach().numpy(), ((0,0), (10,l95-l1+1), (0, 0)), 'constant')\n",
    "        c= x[0]\n",
    "        #print(c.shape)\n",
    "        if img==1:      \n",
    "            b = np.max(c)\n",
    "            a = np.min(c)\n",
    "            c =pxl*(c-a)/(b-a)\n",
    "            c=c.astype(np.uint8)\n",
    "            data=im.fromarray(c)\n",
    "            data = data.resize((height,width) )\n",
    "            arr=np.array(data)\n",
    "            arr = arr.reshape(height,width,1)\n",
    "            x=None\n",
    "        else:        \n",
    "            arr=x[0]  \n",
    "            arr = arr.reshape(arr.shape[0],arr.shape[1],1)\n",
    "            #print(arr.shape)\n",
    "    except Exception as e: # work on python 2.x\n",
    "        #print(\"oops\")\n",
    "        print(str(e))\n",
    "        arr= None     \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 20, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A=np.random.randint(1000, size=(10,20,3))\n",
    "A.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10, 20, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(A, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "TKLK20t_YVGS"
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "# Converting to image\n",
    "def imgconvert(df, l95,DATASET_PATH,height=500, width=64, img=1):\n",
    "    df=df.drop(columns=['level_0'])\n",
    "    df=df.reset_index()\n",
    "    arrlist0=[]\n",
    "    for DATASET_PATH in DATASET_PATHS:\n",
    "        fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']           \n",
    "        if exists(fl)==True:\n",
    "            arrlist0.append(create_arr(fl,l95, height,width,img))\n",
    "        else:\n",
    "            print(fl+\" Does not exists\")\n",
    "    arr= np.concatenate(tuple(arrlist0), axis=2)\n",
    "    #arr = arr.reshape(1, height,width,len(DATASET_PATHS))\n",
    "    arr= np.expand_dims(arr, axis=0)\n",
    "    #print(arr.shape)\n",
    "    #fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']  \n",
    "    #arr=create_arr(fl,l95, height,width,img)\n",
    "    \n",
    "    idx=[]\n",
    "    for i,row in df.iterrows():\n",
    "        #fl = DATASET_PATH+\"/\"+row['name2']\n",
    "        arrlist2=[]\n",
    "        \n",
    "        for DATASET_PATH in DATASET_PATHS:            \n",
    "            fl = DATASET_PATH+\"/\"+row['name2'] \n",
    "            if exists(fl)==True:\n",
    "                arrlist2.append(create_arr(fl,l95, height,width,img)) \n",
    "            else:\n",
    "                print(fl+\" Does not exists\")\n",
    "               \n",
    "        try:\n",
    "            #arr2=create_arr(fl,l95, height,width,img)            \n",
    "            arr2 = np.concatenate(tuple(arrlist2), axis=2)\n",
    "            #arr2 = arr2.reshape(1, height,width,len(DATASET_PATHS))\n",
    "            arr2= np.expand_dims(arr2, axis=0)\n",
    "            arr = np.vstack((arr,arr2))\n",
    "\n",
    "                \n",
    "            idx.append(i)\n",
    "            x=None            \n",
    "        except Exception as e: \n",
    "            #print(\"dhat teri\")\n",
    "            print(str(e))\n",
    "    \n",
    "    arr =np.delete(arr, (0), axis=0) # First row was dummy row\n",
    "    labels=np.array(df.emotion.iloc[idx])\n",
    "    return arr , labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size): \n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X= df\n",
    "y= df['emotion']\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=1\n",
    "val_x, val_y =imgconvert(X_val,l95,DATASET_PATH,img=imgs)\n",
    "test_x, test_y =imgconvert(X_val,l95,DATASET_PATH,img=imgs)\n",
    "val_y=val_y.astype(int) \n",
    "test_y=test_y.astype(int) \n",
    "y_val = to_categorical(val_y)\n",
    "y_test= to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PV5q2H4K4qmq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 500, 64, 3) (8, 2)\n",
      "model does not existing. Creating a new one\n",
      "(500, 64, 3)\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.4623 - categorical_accuracy: 0.3750\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.44186, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 13s 13s/step - loss: 7.4623 - categorical_accuracy: 0.3750 - val_loss: 72883.7422 - val_categorical_accuracy: 0.4419\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 51786.1484 - categorical_accuracy: 0.6250\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 51786.1484 - categorical_accuracy: 0.6250 - val_loss: 422635.2188 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 277342.2188 - categorical_accuracy: 0.6250\n",
      "Epoch 3: val_categorical_accuracy improved from 0.44186 to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 277342.2188 - categorical_accuracy: 0.6250 - val_loss: 152980.6094 - val_categorical_accuracy: 0.5581\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 208244.6562 - categorical_accuracy: 0.3750\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 2s 2s/step - loss: 208244.6562 - categorical_accuracy: 0.3750 - val_loss: 775803.0000 - val_categorical_accuracy: 0.5581\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1017793.8750 - categorical_accuracy: 0.3750\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1017793.8750 - categorical_accuracy: 0.3750 - val_loss: 134529.6094 - val_categorical_accuracy: 0.5581\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 179682.2656 - categorical_accuracy: 0.3750\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 179682.2656 - categorical_accuracy: 0.3750 - val_loss: 14974.9824 - val_categorical_accuracy: 0.4419\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 9639.6309 - categorical_accuracy: 0.6250\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9639.6309 - categorical_accuracy: 0.6250 - val_loss: 22502.6016 - val_categorical_accuracy: 0.4419\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 14532.1953 - categorical_accuracy: 0.6250\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14532.1953 - categorical_accuracy: 0.6250 - val_loss: 14854.6133 - val_categorical_accuracy: 0.5581\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 25332.8789 - categorical_accuracy: 0.3750\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 25332.8789 - categorical_accuracy: 0.3750 - val_loss: 29508.6133 - val_categorical_accuracy: 0.4419\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 19576.6250 - categorical_accuracy: 0.6250\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 19576.6250 - categorical_accuracy: 0.6250 - val_loss: 4188.3481 - val_categorical_accuracy: 0.4419\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2969.3252 - categorical_accuracy: 0.6250\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2969.3252 - categorical_accuracy: 0.6250 - val_loss: 456.8615 - val_categorical_accuracy: 0.4419\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 502.0695 - categorical_accuracy: 0.6250\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 502.0695 - categorical_accuracy: 0.6250 - val_loss: 1497.5455 - val_categorical_accuracy: 0.5581\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1612.0719 - categorical_accuracy: 0.2500\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1612.0719 - categorical_accuracy: 0.2500 - val_loss: 259.5485 - val_categorical_accuracy: 0.4302\n",
      "Epoch 13: early stopping\n",
      "Total memory: 51527024640\n",
      "Free memory: 51046449152\n",
      "Used memory: 480575488\n",
      "(8, 500, 64, 3) (8, 2)\n",
      "Loading existing model\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 200.3688 - categorical_accuracy: 0.7500\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 200.3688 - categorical_accuracy: 0.7500 - val_loss: 780.9520 - val_categorical_accuracy: 0.5581\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1390.8252 - categorical_accuracy: 0.2500\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1390.8252 - categorical_accuracy: 0.2500 - val_loss: 536.7413 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 296.9050 - categorical_accuracy: 0.7500\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 296.9050 - categorical_accuracy: 0.7500 - val_loss: 1224.2720 - val_categorical_accuracy: 0.4419\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 625.4517 - categorical_accuracy: 0.7500\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 625.4517 - categorical_accuracy: 0.7500 - val_loss: 1024.8998 - val_categorical_accuracy: 0.4419\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 482.5020 - categorical_accuracy: 0.7500\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 482.5020 - categorical_accuracy: 0.7500 - val_loss: 141.4428 - val_categorical_accuracy: 0.4419\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 52.0572 - categorical_accuracy: 0.7500\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 52.0572 - categorical_accuracy: 0.7500 - val_loss: 1487.5488 - val_categorical_accuracy: 0.5581\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2727.0195 - categorical_accuracy: 0.2500\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2727.0195 - categorical_accuracy: 0.2500 - val_loss: 340.1517 - val_categorical_accuracy: 0.5581\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 646.6431 - categorical_accuracy: 0.2500\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 646.6431 - categorical_accuracy: 0.2500 - val_loss: 2402.8850 - val_categorical_accuracy: 0.4419\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1168.2505 - categorical_accuracy: 0.7500\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1168.2505 - categorical_accuracy: 0.7500 - val_loss: 3938.8875 - val_categorical_accuracy: 0.4419\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1935.4507 - categorical_accuracy: 0.7500\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1935.4507 - categorical_accuracy: 0.7500 - val_loss: 4562.1133 - val_categorical_accuracy: 0.4419\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2253.5076 - categorical_accuracy: 0.7500\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2253.5076 - categorical_accuracy: 0.7500 - val_loss: 4607.2637 - val_categorical_accuracy: 0.4419\n",
      "Epoch 11: early stopping\n",
      "Total memory: 51527024640\n",
      "Free memory: 51046449152\n",
      "Used memory: 480575488\n",
      "(8, 500, 64, 3) (8, 1)\n",
      "Loading existing model\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 8306.7129 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.44186, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 13s 13s/step - loss: 8306.7129 - categorical_accuracy: 0.0000e+00 - val_loss: 4010.5806 - val_categorical_accuracy: 0.4419\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 7252.2935 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7252.2935 - categorical_accuracy: 0.0000e+00 - val_loss: 3042.4180 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 5525.3818 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5525.3818 - categorical_accuracy: 0.0000e+00 - val_loss: 1906.4669 - val_categorical_accuracy: 0.4419\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 3488.8452 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3488.8452 - categorical_accuracy: 0.0000e+00 - val_loss: 736.0856 - val_categorical_accuracy: 0.4419\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1366.4535 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 5: val_categorical_accuracy improved from 0.44186 to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 1366.4535 - categorical_accuracy: 0.0000e+00 - val_loss: 312.4431 - val_categorical_accuracy: 0.5581\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 657.3507 - categorical_accuracy: 1.0000\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 2s 2s/step - loss: 657.3507 - categorical_accuracy: 1.0000 - val_loss: 779.5596 - val_categorical_accuracy: 0.5581\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1726.9857 - categorical_accuracy: 1.0000\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1726.9857 - categorical_accuracy: 1.0000 - val_loss: 977.1143 - val_categorical_accuracy: 0.5581\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2206.4890 - categorical_accuracy: 1.0000\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2206.4890 - categorical_accuracy: 1.0000 - val_loss: 1018.9156 - val_categorical_accuracy: 0.5581\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2279.2139 - categorical_accuracy: 1.0000\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2279.2139 - categorical_accuracy: 1.0000 - val_loss: 964.0459 - val_categorical_accuracy: 0.5349\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1998.4021 - categorical_accuracy: 1.0000\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1998.4021 - categorical_accuracy: 1.0000 - val_loss: 700.3043 - val_categorical_accuracy: 0.5349\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1250.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1250.4852 - categorical_accuracy: 1.0000 - val_loss: 211.1683 - val_categorical_accuracy: 0.4302\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 214.6814 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 214.6814 - categorical_accuracy: 0.0000e+00 - val_loss: 597.0317 - val_categorical_accuracy: 0.4419\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1067.5417 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1067.5417 - categorical_accuracy: 0.0000e+00 - val_loss: 545.1239 - val_categorical_accuracy: 0.4419\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 980.4429 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 980.4429 - categorical_accuracy: 0.0000e+00 - val_loss: 590.2380 - val_categorical_accuracy: 0.5581\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1308.0518 - categorical_accuracy: 1.0000\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1308.0518 - categorical_accuracy: 1.0000 - val_loss: 1181.7291 - val_categorical_accuracy: 0.5581\n",
      "Epoch 15: early stopping\n",
      "Total memory: 51527024640\n",
      "Free memory: 51046449152\n",
      "Used memory: 480575488\n",
      "(8, 500, 64, 3) (8, 2)\n",
      "Loading existing model\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1271.9629 - categorical_accuracy: 0.5000\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.44186, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 1271.9629 - categorical_accuracy: 0.5000 - val_loss: 8573.7539 - val_categorical_accuracy: 0.4419\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 7848.1211 - categorical_accuracy: 0.5000\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7848.1211 - categorical_accuracy: 0.5000 - val_loss: 4273.7344 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 3902.7812 - categorical_accuracy: 0.5000\n",
      "Epoch 3: val_categorical_accuracy improved from 0.44186 to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 8\n",
    "chunks=split_dataframe(X_train,chunk_size)\n",
    "\n",
    "name='shallow'\n",
    "modname ='saved_model/'+therapist+'_'+emotion+'_'+name\n",
    "#model = load_model(modname)\n",
    "model=None\n",
    "epochs=150\n",
    "\n",
    "for chunk in chunks[0:len(chunks)]:\n",
    "    chunk=chunk.drop(columns=['level_0'])\n",
    "    chunk=chunk.reset_index()\n",
    "    try:\n",
    "        train_x, train_y =imgconvert(chunk,l95,DATASET_PATH,img=imgs)       \n",
    "        # Train and Test\n",
    "        train_y=train_y.astype(int)       \n",
    "        # one hot encode outputs\n",
    "        y_train = to_categorical(train_y)        \n",
    "        print(train_x.shape, y_train.shape)\n",
    "        model= mcs.shallow_model( model,therapist.replace(\" \",\"\"), emotion, train_x, y_train, val_x, y_val, train_x.shape[0],  modname, name, epochs  )      \n",
    "      \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#modname ='saved_model/'+therapist+'_'+emotion+'_'+name #+'.h5'\n",
    "#model = load_model(modname)\n",
    "score = model.evaluate(test_x, y_test, verbose=1)\n",
    "\n",
    "#print loss and accuracy\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "print(os.system('!nvidia-smi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= arr[0].reshape(416, 64,1)\n",
    "b=arr[1].reshape(416, 64,1)\n",
    "c=arr[2].reshape(416, 64,1)\n",
    "d=tuple([a,b,c])\n",
    "e= np.concatenate(d, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = np.dstack((abc,abc,abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc =arr.reshape(44, 416, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "usingw2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchaudio==0.10.1\n",
      "  Downloading torchaudio-0.10.1-cp38-cp38-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attention\n",
      "  Downloading attention-4.1-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (5.3.1)\n",
      "Requirement already satisfied: h5py in /usr/lib/python3/dist-packages (2.10.0)\n",
      "Collecting nvidia_smi\n",
      "  Downloading nvidia_smi-0.1.3-py36-none-any.whl (11 kB)\n",
      "Collecting keras==2.9.0rc1\n",
      "  Downloading keras-2.9.0rc1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.6 MB 78.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow\n",
      "  Downloading tensorflow-2.9.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
      "\u001b[K     |██████████████████▉             | 302.0 MB 104.7 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 511.7 MB 85 kB/s /s eta 0:00:01\n",
      "\u001b[?25hCollecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[K     |████████████████████████████████| 214 kB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (3.5.2)\n",
      "Collecting pickle5\n",
      "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 92.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: Pillow in /usr/lib/python3/dist-packages (7.0.0)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (0.25.3)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.8/site-packages (1.22.4)\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.9 MB 81.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numba\n",
      "  Downloading numba-0.55.2-cp38-cp38-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 85.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.10.1\n",
      "  Downloading torch-1.10.1-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[K     |████████████████▉               | 463.7 MB 113.5 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |█████████████████████████████▏  | 804.5 MB 34.4 MB/s eta 0:00:031��████████████████████           | 576.8 MB 110.1 MB/s eta 0:00:03"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 881.9 MB 24 kB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from nvidia_smi) (1.14.0)\n",
      "Collecting pytest>=4.3.1\n",
      "  Downloading pytest-7.1.2-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 5.0 MB/s eta 0:00:01�█████████▏        | 215 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sorcery>=0.1.0\n",
      "  Downloading sorcery-0.2.2-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/lib/python3/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/lib/python3/dist-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/lib/python3/dist-packages (from tensorflow) (1.29.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.11.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/lib/python3/dist-packages (from tensorflow) (3.3.0)\n",
      "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
      "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 71.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.10,>=2.9\n",
      "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 80.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/lib/python3/dist-packages (from tensorflow) (20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorflow) (4.2.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-14.0.1-py2.py3-none-manylinux1_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.5 MB 59.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.6.2)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.1.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 90.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/lib/python3/dist-packages (from tensorflow) (0.2.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.26.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 47.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.10 in /usr/lib/python3/dist-packages (from librosa) (4.4.2)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.2.2.tar.gz (323 kB)\n",
      "\u001b[K     |████████████████████████████████| 323 kB 64.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soundfile>=0.10.2\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from librosa) (1.8.1)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-2.1.9.tar.gz (377 kB)\n",
      "\u001b[K     |████████████████████████████████| 377 kB 61.2 MB/s eta 0:00:01��█████████████             | 225 kB 61.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/lib/python3/dist-packages (from librosa) (0.22.2.post1)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 5.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.10.0)\n",
      "Collecting llvmlite<0.39,>=0.38.0rc1\n",
      "  Downloading llvmlite-0.38.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 83.5 MB/s eta 0:00:01     |▎                               | 276 kB 83.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[K     |████████████████████████████████| 98 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from pytest>=4.3.1->nvidia_smi) (19.3.0)\n",
      "Collecting littleutils>=0.2.1\n",
      "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
      "Collecting executing\n",
      "  Downloading executing-0.8.3-py2.py3-none-any.whl (16 kB)\n",
      "Collecting asttokens\n",
      "  Downloading asttokens-2.0.5-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.9 MB 93.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.1.2-py3-none-any.whl (224 kB)\n",
      "\u001b[K     |████████████████████████████████| 224 kB 89.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.34.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.8.0-py2.py3-none-any.whl (164 kB)\n",
      "\u001b[K     |████████████████████████████████| 164 kB 87.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "\u001b[K     |████████████████████████████████| 781 kB 66.1 MB/s eta 0:00:01�████                      | 245 kB 66.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in /usr/lib/python3/dist-packages (from pooch>=1.0->librosa) (1.4.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /home/ubuntu/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: pycparser in /home/ubuntu/.local/lib/python3.8/site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
      "Building wheels for collected packages: pickle5, resampy, audioread, littleutils\n",
      "  Building wheel for pickle5 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pickle5: filename=pickle5-0.0.11-cp38-cp38-linux_x86_64.whl size=262074 sha256=37343512e2ec40eeaa8812dc57b59c19a73106b739fa1649298c06ad1bef071f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/25/d4/61/dbd8edd1a0d656be7b4267c85db3b61951eb60016a0154a122\n",
      "  Building wheel for resampy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for resampy: filename=resampy-0.2.2-py3-none-any.whl size=320717 sha256=e86881930af8a7becf7235f560af8e64fb6b0071ba11bfb205d03bf2eeaea356\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/6f/d1/5d/f13da53b1dcbc2624ff548456c9ffb526c914f53c12c318bb4\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-2.1.9-py3-none-any.whl size=23142 sha256=512ecbc6dd2c69cd7d07c53b5a85afefcfb2237c2df1bac3b96de6d296f87cb6\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/49/5a/e4/df590783499a992a88de6c0898991d1167453a3196d0d1eeb7\n",
      "  Building wheel for littleutils (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=4f668da6d07728f7a67abfb7364ea3625117e35af5b8926bd086ad021841ce3f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/6a/33/c4/0ef84d7f5568c2823e3d63a6e08988852fb9e4bc822034870a\n",
      "Successfully built pickle5 resampy audioread littleutils\n",
      "Installing collected packages: torch, torchaudio, keras, tensorflow-estimator, absl-py, tensorboard-data-server, werkzeug, google-auth, tensorboard-plugin-wit, tensorboard, libclang, tensorflow-io-gcs-filesystem, tensorflow, attention, pluggy, tomli, py, iniconfig, pytest, littleutils, executing, asttokens, sorcery, nvidia-smi, llvmlite, numba, resampy, soundfile, audioread, pooch, librosa, pickle5, opencv-python\n",
      "\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/home/ubuntu/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed absl-py-1.1.0 asttokens-2.0.5 attention-4.1 audioread-2.1.9 executing-0.8.3 google-auth-2.8.0 iniconfig-1.1.1 keras-2.9.0rc1 libclang-14.0.1 librosa-0.9.2 littleutils-0.2.2 llvmlite-0.38.1 numba-0.55.2 nvidia-smi-0.1.3 opencv-python-4.6.0.66 pickle5-0.0.11 pluggy-1.0.0 pooch-1.6.0 py-1.11.0 pytest-7.1.2 resampy-0.2.2 sorcery-0.2.2 soundfile-0.10.3.post1 tensorboard-2.9.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-io-gcs-filesystem-0.26.0 tomli-2.0.1 torch-1.10.1 torchaudio-0.10.1 werkzeug-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torchaudio==0.10.1 attention pyyaml h5py nvidia_smi keras==2.9.0rc1 tensorflow librosa matplotlib pickle5 Pillow pandas numpy opencv-python numba #gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.docs import doc_controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6cTyZVydgca",
    "outputId": "222bd80d-a8df-41e6-cb7b-316a13a7e8d2"
   },
   "outputs": [],
   "source": [
    "import librosa, librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import pickle5 as pickle\n",
    "from PIL import Image as im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4NblImFddp9T"
   },
   "outputs": [],
   "source": [
    "pathG='../data' \n",
    "DATASET_PATH0 = pathG+\"/w2v2/w2v2L0\"\n",
    "DATASET_PATH4 = pathG+\"/w2v2/w2v2L4\"\n",
    "DATASET_PATH8 = pathG+\"/w2v2/w2v2L8\"\n",
    "DATASET_PATHS = [DATASET_PATH0,DATASET_PATH4,DATASET_PATH8]\n",
    "DATASET_PATH = DATASET_PATHS[0]\n",
    "#https://stackoverflow.com/questions/10443295/combine-3-separate-numpy-arrays-to-an-rgb-image-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "therapist='Yared Alemu'\n",
    "emotion='fear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dyfp0YeCwBMJ"
   },
   "outputs": [],
   "source": [
    "# Get data list\n",
    "\"\"\"\n",
    "Created on Tue Jun  7 20:48:17 2022\n",
    "\n",
    "@author: sanjeev\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization #, regularizers\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import models_custom as mcs\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "#os.chdir('/media/sanjeev/Data/Pract/Practicum/codesNdata/mycode')\n",
    "#pathG='../data'\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(pathG+\"/labels/combined_csv.csv\")\n",
    "\n",
    "df = df.reset_index()\n",
    "df['name2'] =df['name'].apply (lambda x: x.split(\".\")[0]+\".pickle\")\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "import os\n",
    " \n",
    "filenames= glob.glob(DATASET_PATH+\"/*.pickle\" )\n",
    "filenames = [os.path.basename(x) for x in filenames]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>level_0</th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_type</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>therapist</th>\n",
       "      <th>name2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>6382_53113_1573430400.wav</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>6382_53113_1573430400.pickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>4645_53113_1587081600.wav</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>4645_53113_1587081600.pickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>71521_72285_2140228092.wav</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>71521_72285_2140228092.pickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>49646_53796_1571270400.wav</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>49646_53796_1571270400.pickle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>1940_39117_1598745600.wav</td>\n",
       "      <td>none</td>\n",
       "      <td>none</td>\n",
       "      <td>1940_39117_1598745600.pickle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   level_0  index  Unnamed: 0  emotion emotion_type  \\\n",
       "0        0      0           0        0         none   \n",
       "1        1      1           0        0         none   \n",
       "2        2      2           0        0         none   \n",
       "3        3      3           0        0         none   \n",
       "4        4      4           0        0         none   \n",
       "\n",
       "                         name rating therapist                          name2  \n",
       "0   6382_53113_1573430400.wav   none      none   6382_53113_1573430400.pickle  \n",
       "1   4645_53113_1587081600.wav   none      none   4645_53113_1587081600.pickle  \n",
       "2  71521_72285_2140228092.wav   none      none  71521_72285_2140228092.pickle  \n",
       "3  49646_53796_1571270400.wav   none      none  49646_53796_1571270400.pickle  \n",
       "4   1940_39117_1598745600.wav   none      none   1940_39117_1598745600.pickle  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "TTNcFKd2FNaG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is to delete the rows greater than max size\n",
    "l95=5000\n",
    "df2=df.copy(deep=True)\n",
    "for i,row in df.iterrows():    \n",
    "    fl = DATASET_PATH+\"/\"+row['name2']\n",
    "    try:\n",
    "        with open(fl,\"rb\") as f:\n",
    "            x=pickle.load(f)     \n",
    "        l1 = x[0].shape[0]\n",
    "        w1 = x[0].shape[1] \n",
    "        if l1> l95:\n",
    "            try:\n",
    "            #print(i)\n",
    "                df2=df2.drop(df.iloc[i].name)\n",
    "            except:\n",
    "                print(i,l1)\n",
    "                print(\"encountered and error\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "imgconvert(df, l95,DATASET_PATHS,height=224, width=224, img=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "results = Parallel(n_jobs=-1)(delayed(process)(recrd) for recrd in fll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fll=[]\n",
    "for i,row in df.iterrows():    \n",
    "    fl = DATASET_PATH+\"/\"+row['name2']\n",
    "    fll.append(fl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z_Nby6tPCYiM"
   },
   "outputs": [],
   "source": [
    "# Normalizing data and creating an array\n",
    "def create_arr(fl,l95, height,width,img):\n",
    "    if img==1:\n",
    "        pxl=255\n",
    "    else:\n",
    "        pxl=1  \n",
    "    with open(fl,\"rb\") as f:\n",
    "        x=pickle.load(f) \n",
    "    l1 = x[0].shape[0]\n",
    "    try:\n",
    "        #print(l95-l1+1)\n",
    "        x=np.pad(x.cpu().detach().numpy(), ((0,0), (10,l95-l1+1), (0, 0)), 'constant')\n",
    "        c= x[0]\n",
    "        #print(c.shape)\n",
    "        if img==1:      \n",
    "            b = np.max(c)\n",
    "            a = np.min(c)\n",
    "            c =pxl*(c-a)/(b-a)\n",
    "            c=c.astype(np.uint8)\n",
    "            data=im.fromarray(c)\n",
    "            data = data.resize((height,width) )\n",
    "            arr=np.array(data)\n",
    "            arr = arr.reshape(height,width,1)\n",
    "            x=None\n",
    "        else:        \n",
    "            arr=x[0]  \n",
    "            arr = arr.reshape(arr.shape[0],arr.shape[1],1)\n",
    "            #print(arr.shape)\n",
    "    except Exception as e: # work on python 2.x\n",
    "        #print(\"oops\")\n",
    "        print(str(e))\n",
    "        arr= None     \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3af44e6bdddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimgconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDATASET_PATHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-84a2c9d05179>\u001b[0m in \u001b[0;36mimgconvert\u001b[0;34m(df, l95, DATASET_PATHS, height, width, img)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mfl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDATASET_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0marrlist2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\" Does not exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d35cd905f07d>\u001b[0m in \u001b[0;36mcreate_arr\u001b[0;34m(fl, l95, height, width, img)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpxl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0ml1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "imgconvert(df, l95,DATASET_PATHS,height=224, width=224, img=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "TKLK20t_YVGS"
   },
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "# Converting to image\n",
    "def imgconvert(df, l95,DATASET_PATHS,height=224, width=224, img=1):\n",
    "    df=df.drop(columns=['level_0'])\n",
    "    df=df.reset_index()\n",
    "    arrlist0=[]\n",
    "    for DATASET_PATH in DATASET_PATHS:\n",
    "        fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']           \n",
    "        if exists(fl)==True:\n",
    "            arrlist0.append(create_arr(fl,l95, height,width,img))\n",
    "        else:\n",
    "            print(fl+\" Does not exists\")\n",
    "    arr= np.concatenate(tuple(arrlist0), axis=2)\n",
    "    #arr = arr.reshape(1, height,width,len(DATASET_PATHS))\n",
    "    arr= np.expand_dims(arr, axis=0)\n",
    "    #print(arr.shape)\n",
    "    #fl= DATASET_PATH +\"/\"+df.iloc[0]['name2']  \n",
    "    #arr=create_arr(fl,l95, height,width,img)\n",
    "    \n",
    "    idx=[]\n",
    "    for i,row in df.iterrows():\n",
    "        #fl = DATASET_PATH+\"/\"+row['name2']\n",
    "        arrlist2=[]\n",
    "        \n",
    "        for DATASET_PATH in DATASET_PATHS:            \n",
    "            fl = DATASET_PATH+\"/\"+row['name2'] \n",
    "            if exists(fl)==True:\n",
    "                arrlist2.append(create_arr(fl,l95, height,width,img)) \n",
    "            else:\n",
    "                print(fl+\" Does not exists\")\n",
    "               \n",
    "        try:\n",
    "            #arr2=create_arr(fl,l95, height,width,img)            \n",
    "            arr2 = np.concatenate(tuple(arrlist2), axis=2)\n",
    "            #arr2 = arr2.reshape(1, height,width,len(DATASET_PATHS))\n",
    "            arr2= np.expand_dims(arr2, axis=0)\n",
    "            arr = np.vstack((arr,arr2))\n",
    "\n",
    "                \n",
    "            idx.append(i)\n",
    "            x=None            \n",
    "        except Exception as e: \n",
    "            #print(\"dhat teri\")\n",
    "            print(str(e))\n",
    "    \n",
    "    arr =np.delete(arr, (0), axis=0) # First row was dummy row\n",
    "    flnm='../data/w2imgs/w2.pkl #'+row['name2']\n",
    "    with open(flnm, 'wb') as handle:\n",
    "        pickle.dump(arr, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    labels=np.array(df.emotion.iloc[idx])\n",
    "    return arr , labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/w2v2/w2v2L0', '../data/w2v2/w2v2L4', '../data/w2v2/w2v2L8']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "fl0=list(df['name2'].unique())\n",
    "def pklc(flname):\n",
    "#for i,row in df.iterrows():        \n",
    "    arrlist2=[]        \n",
    "    for DATASET_PATH in DATASET_PATHS:            \n",
    "        fl = DATASET_PATH+\"/\"+flname \n",
    "        if exists(fl)==True:\n",
    "            arrlist2.append(create_arr(fl,l95, 224,224,1)) \n",
    "        else:\n",
    "            print(fl+\" Does not exists\")\n",
    "    try:\n",
    "        #arr2=create_arr(fl,l95, height,width,img)            \n",
    "        arr2 = np.concatenate(tuple(arrlist2), axis=2)\n",
    "        #arr2 = arr2.reshape(1, height,width,len(DATASET_PATHS))\n",
    "        #arr2= np.expand_dims(arr2, axis=0)\n",
    "        print(arr2.shape)\n",
    "        flnm='../data/w2vimgs/'+flname.split('.')[0]+'.jpeg'\n",
    "        im = Image.fromarray(arr2)\n",
    "        im.save(flnm)\n",
    "        #with open(flnm, 'wb') as handle:\n",
    "            #pickle.dump(arr2, handle, protocol=pickle.HIGHEST_PROTOCOL)           \n",
    "    except Exception as e: \n",
    "        #print(\"dhat teri\")\n",
    "        print(str(e))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6382_53113_1573430400.png'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flname='6382_53113_1573430400.pickle'\n",
    "flname.split('.')[0]+'.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "results = Parallel(n_jobs=-1)(delayed(pklc)(flname) for flname in fl0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "pklc('6382_53113_1573430400.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, chunk_size): \n",
    "    chunks = list()\n",
    "    num_chunks = len(df) // chunk_size + 1\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X= df\n",
    "y= df['emotion']\n",
    "X_train, X_test1, y_train, y_test1 = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test1, y_test1, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs=1\n",
    "val_x, val_y =imgconvert(X_val,l95,DATASET_PATH,img=imgs)\n",
    "test_x, test_y =imgconvert(X_val,l95,DATASET_PATH,img=imgs)\n",
    "val_y=val_y.astype(int) \n",
    "test_y=test_y.astype(int) \n",
    "y_val = to_categorical(val_y)\n",
    "y_test= to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PV5q2H4K4qmq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 500, 64, 3) (8, 2)\n",
      "model does not existing. Creating a new one\n",
      "(500, 64, 3)\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.4623 - categorical_accuracy: 0.3750\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.44186, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 13s 13s/step - loss: 7.4623 - categorical_accuracy: 0.3750 - val_loss: 72883.7422 - val_categorical_accuracy: 0.4419\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 51786.1484 - categorical_accuracy: 0.6250\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 51786.1484 - categorical_accuracy: 0.6250 - val_loss: 422635.2188 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 277342.2188 - categorical_accuracy: 0.6250\n",
      "Epoch 3: val_categorical_accuracy improved from 0.44186 to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 277342.2188 - categorical_accuracy: 0.6250 - val_loss: 152980.6094 - val_categorical_accuracy: 0.5581\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 208244.6562 - categorical_accuracy: 0.3750\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 2s 2s/step - loss: 208244.6562 - categorical_accuracy: 0.3750 - val_loss: 775803.0000 - val_categorical_accuracy: 0.5581\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1017793.8750 - categorical_accuracy: 0.3750\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1017793.8750 - categorical_accuracy: 0.3750 - val_loss: 134529.6094 - val_categorical_accuracy: 0.5581\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 179682.2656 - categorical_accuracy: 0.3750\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 179682.2656 - categorical_accuracy: 0.3750 - val_loss: 14974.9824 - val_categorical_accuracy: 0.4419\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 9639.6309 - categorical_accuracy: 0.6250\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 9639.6309 - categorical_accuracy: 0.6250 - val_loss: 22502.6016 - val_categorical_accuracy: 0.4419\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 14532.1953 - categorical_accuracy: 0.6250\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 14532.1953 - categorical_accuracy: 0.6250 - val_loss: 14854.6133 - val_categorical_accuracy: 0.5581\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 25332.8789 - categorical_accuracy: 0.3750\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 25332.8789 - categorical_accuracy: 0.3750 - val_loss: 29508.6133 - val_categorical_accuracy: 0.4419\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 19576.6250 - categorical_accuracy: 0.6250\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 19576.6250 - categorical_accuracy: 0.6250 - val_loss: 4188.3481 - val_categorical_accuracy: 0.4419\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2969.3252 - categorical_accuracy: 0.6250\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2969.3252 - categorical_accuracy: 0.6250 - val_loss: 456.8615 - val_categorical_accuracy: 0.4419\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 502.0695 - categorical_accuracy: 0.6250\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 502.0695 - categorical_accuracy: 0.6250 - val_loss: 1497.5455 - val_categorical_accuracy: 0.5581\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1612.0719 - categorical_accuracy: 0.2500\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1612.0719 - categorical_accuracy: 0.2500 - val_loss: 259.5485 - val_categorical_accuracy: 0.4302\n",
      "Epoch 13: early stopping\n",
      "Total memory: 51527024640\n",
      "Free memory: 51046449152\n",
      "Used memory: 480575488\n",
      "(8, 500, 64, 3) (8, 2)\n",
      "Loading existing model\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 200.3688 - categorical_accuracy: 0.7500\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 200.3688 - categorical_accuracy: 0.7500 - val_loss: 780.9520 - val_categorical_accuracy: 0.5581\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1390.8252 - categorical_accuracy: 0.2500\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1390.8252 - categorical_accuracy: 0.2500 - val_loss: 536.7413 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 296.9050 - categorical_accuracy: 0.7500\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 296.9050 - categorical_accuracy: 0.7500 - val_loss: 1224.2720 - val_categorical_accuracy: 0.4419\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 625.4517 - categorical_accuracy: 0.7500\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 625.4517 - categorical_accuracy: 0.7500 - val_loss: 1024.8998 - val_categorical_accuracy: 0.4419\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 482.5020 - categorical_accuracy: 0.7500\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 482.5020 - categorical_accuracy: 0.7500 - val_loss: 141.4428 - val_categorical_accuracy: 0.4419\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 52.0572 - categorical_accuracy: 0.7500\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 52.0572 - categorical_accuracy: 0.7500 - val_loss: 1487.5488 - val_categorical_accuracy: 0.5581\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2727.0195 - categorical_accuracy: 0.2500\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2727.0195 - categorical_accuracy: 0.2500 - val_loss: 340.1517 - val_categorical_accuracy: 0.5581\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 646.6431 - categorical_accuracy: 0.2500\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 646.6431 - categorical_accuracy: 0.2500 - val_loss: 2402.8850 - val_categorical_accuracy: 0.4419\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1168.2505 - categorical_accuracy: 0.7500\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1168.2505 - categorical_accuracy: 0.7500 - val_loss: 3938.8875 - val_categorical_accuracy: 0.4419\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1935.4507 - categorical_accuracy: 0.7500\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1935.4507 - categorical_accuracy: 0.7500 - val_loss: 4562.1133 - val_categorical_accuracy: 0.4419\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2253.5076 - categorical_accuracy: 0.7500\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2253.5076 - categorical_accuracy: 0.7500 - val_loss: 4607.2637 - val_categorical_accuracy: 0.4419\n",
      "Epoch 11: early stopping\n",
      "Total memory: 51527024640\n",
      "Free memory: 51046449152\n",
      "Used memory: 480575488\n",
      "(8, 500, 64, 3) (8, 1)\n",
      "Loading existing model\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 8306.7129 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.44186, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 13s 13s/step - loss: 8306.7129 - categorical_accuracy: 0.0000e+00 - val_loss: 4010.5806 - val_categorical_accuracy: 0.4419\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 7252.2935 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7252.2935 - categorical_accuracy: 0.0000e+00 - val_loss: 3042.4180 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 5525.3818 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 5525.3818 - categorical_accuracy: 0.0000e+00 - val_loss: 1906.4669 - val_categorical_accuracy: 0.4419\n",
      "Epoch 4/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 3488.8452 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 4: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 3488.8452 - categorical_accuracy: 0.0000e+00 - val_loss: 736.0856 - val_categorical_accuracy: 0.4419\n",
      "Epoch 5/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1366.4535 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 5: val_categorical_accuracy improved from 0.44186 to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 1366.4535 - categorical_accuracy: 0.0000e+00 - val_loss: 312.4431 - val_categorical_accuracy: 0.5581\n",
      "Epoch 6/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 657.3507 - categorical_accuracy: 1.0000\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 2s 2s/step - loss: 657.3507 - categorical_accuracy: 1.0000 - val_loss: 779.5596 - val_categorical_accuracy: 0.5581\n",
      "Epoch 7/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1726.9857 - categorical_accuracy: 1.0000\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1726.9857 - categorical_accuracy: 1.0000 - val_loss: 977.1143 - val_categorical_accuracy: 0.5581\n",
      "Epoch 8/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2206.4890 - categorical_accuracy: 1.0000\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2206.4890 - categorical_accuracy: 1.0000 - val_loss: 1018.9156 - val_categorical_accuracy: 0.5581\n",
      "Epoch 9/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 2279.2139 - categorical_accuracy: 1.0000\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2279.2139 - categorical_accuracy: 1.0000 - val_loss: 964.0459 - val_categorical_accuracy: 0.5349\n",
      "Epoch 10/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1998.4021 - categorical_accuracy: 1.0000\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1998.4021 - categorical_accuracy: 1.0000 - val_loss: 700.3043 - val_categorical_accuracy: 0.5349\n",
      "Epoch 11/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1250.4852 - categorical_accuracy: 1.0000\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1250.4852 - categorical_accuracy: 1.0000 - val_loss: 211.1683 - val_categorical_accuracy: 0.4302\n",
      "Epoch 12/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 214.6814 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 214.6814 - categorical_accuracy: 0.0000e+00 - val_loss: 597.0317 - val_categorical_accuracy: 0.4419\n",
      "Epoch 13/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1067.5417 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1067.5417 - categorical_accuracy: 0.0000e+00 - val_loss: 545.1239 - val_categorical_accuracy: 0.4419\n",
      "Epoch 14/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 980.4429 - categorical_accuracy: 0.0000e+00\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 980.4429 - categorical_accuracy: 0.0000e+00 - val_loss: 590.2380 - val_categorical_accuracy: 0.5581\n",
      "Epoch 15/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1308.0518 - categorical_accuracy: 1.0000\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.55814\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1308.0518 - categorical_accuracy: 1.0000 - val_loss: 1181.7291 - val_categorical_accuracy: 0.5581\n",
      "Epoch 15: early stopping\n",
      "Total memory: 51527024640\n",
      "Free memory: 51046449152\n",
      "Used memory: 480575488\n",
      "(8, 500, 64, 3) (8, 2)\n",
      "Loading existing model\n",
      "Epoch 1/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 1271.9629 - categorical_accuracy: 0.5000\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.44186, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/Yared Alemu_fear_shallow/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 12s 12s/step - loss: 1271.9629 - categorical_accuracy: 0.5000 - val_loss: 8573.7539 - val_categorical_accuracy: 0.4419\n",
      "Epoch 2/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 7848.1211 - categorical_accuracy: 0.5000\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.44186\n",
      "1/1 [==============================] - 1s 1s/step - loss: 7848.1211 - categorical_accuracy: 0.5000 - val_loss: 4273.7344 - val_categorical_accuracy: 0.4419\n",
      "Epoch 3/150\n",
      "1/1 [==============================] - ETA: 0s - loss: 3902.7812 - categorical_accuracy: 0.5000\n",
      "Epoch 3: val_categorical_accuracy improved from 0.44186 to 0.55814, saving model to saved_model/Yared Alemu_fear_shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 8\n",
    "chunks=split_dataframe(X_train,chunk_size)\n",
    "\n",
    "name='shallow'\n",
    "modname ='saved_model/'+therapist+'_'+emotion+'_'+name\n",
    "#model = load_model(modname)\n",
    "model=None\n",
    "epochs=150\n",
    "\n",
    "for chunk in chunks[0:len(chunks)]:\n",
    "    chunk=chunk.drop(columns=['level_0'])\n",
    "    chunk=chunk.reset_index()\n",
    "    try:\n",
    "        train_x, train_y =imgconvert(chunk,l95,DATASET_PATH,img=imgs)       \n",
    "        # Train and Test\n",
    "        train_y=train_y.astype(int)       \n",
    "        # one hot encode outputs\n",
    "        y_train = to_categorical(train_y)        \n",
    "        print(train_x.shape, y_train.shape)\n",
    "        model= mcs.shallow_model( model,therapist.replace(\" \",\"\"), emotion, train_x, y_train, val_x, y_val, train_x.shape[0],  modname, name, epochs  )      \n",
    "      \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#modname ='saved_model/'+therapist+'_'+emotion+'_'+name #+'.h5'\n",
    "#model = load_model(modname)\n",
    "score = model.evaluate(test_x, y_test, verbose=1)\n",
    "\n",
    "#print loss and accuracy\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "print(os.system('!nvidia-smi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
    "\n",
    "print(\"Total memory:\", info.total)\n",
    "print(\"Free memory:\", info.free)\n",
    "print(\"Used memory:\", info.used)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= arr[0].reshape(416, 64,1)\n",
    "b=arr[1].reshape(416, 64,1)\n",
    "c=arr[2].reshape(416, 64,1)\n",
    "d=tuple([a,b,c])\n",
    "e= np.concatenate(d, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb = np.dstack((abc,abc,abc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc =arr.reshape(44, 416, 1, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "usingw2vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
